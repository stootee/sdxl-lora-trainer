{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAuSTF8a_D9p"
      },
      "source": [
        "# SDXL LoRA Trainer for Likeness\n",
        "\n",
        "This notebook trains a LoRA (Low-Rank Adaptation) on SDXL to capture your likeness for use in image generation.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with GPU runtime (T4 compatible)\n",
        "- Training images stored in Google Drive with matching .txt caption files\n",
        "- ~15-30 high-quality photos of the subject\n",
        "\n",
        "**Training Data Format:**\n",
        "```\n",
        "your_folder/\n",
        "  image001.jpg\n",
        "  image001.txt  (caption describing the image)\n",
        "  image002.png\n",
        "  image002.txt\n",
        "  ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raBD87aL_D9x"
      },
      "source": [
        "## 1. Setup & Installation\n",
        "\n",
        "Install kohya_ss sd-scripts and dependencies. This takes a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2XGj0pCy_D9y",
        "outputId": "f11eb351-d077-4ea0-ee22-b0a7e9876aac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4, 15360 MiB\n",
            "Found existing installation: torch 2.9.0+cu126\n",
            "Uninstalling torch-2.9.0+cu126:\n",
            "  Successfully uninstalled torch-2.9.0+cu126\n",
            "Found existing installation: torchvision 0.24.0+cu126\n",
            "Uninstalling torchvision-0.24.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.24.0+cu126\n",
            "Found existing installation: torchaudio 2.9.0+cu126\n",
            "Uninstalling torchaudio-2.9.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.4.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (798.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.19.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.4.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (75.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.1+cu121) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.1+cu121) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1+cu121) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.1+cu121) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.1+cu121) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting xformers==0.0.28.post1\n",
            "  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers==0.0.28.post1) (2.0.2)\n",
            "Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.12/dist-packages (from xformers==0.0.28.post1) (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1->xformers==0.0.28.post1) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->xformers==0.0.28.post1) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.1->xformers==0.0.28.post1) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.1->xformers==0.0.28.post1) (1.3.0)\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.28.post1\n",
            "Cloning into '/content/sd-scripts'...\n",
            "remote: Enumerating objects: 10791, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 10791 (delta 55), reused 29 (delta 29), pack-reused 10698 (from 3)\u001b[K\n",
            "Receiving objects: 100% (10791/10791), 13.08 MiB | 16.91 MiB/s, done.\n",
            "Resolving deltas: 100% (7700/7700), done.\n",
            "/content/sd-scripts\n",
            "Branch 'sd3' set up to track remote branch 'sd3' from 'origin'.\n",
            "Switched to a new branch 'sd3'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m137.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.4/273.4 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.8/434.8 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for schedulefree (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.33.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.3/76.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "==================================================\n",
            "Installation complete!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 1.1 Install Dependencies\n",
        "#@markdown Run this cell first. Takes ~3-5 minutes.\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Check GPU\n",
        "gpu_info = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
        "                          capture_output=True, text=True)\n",
        "print(f\"GPU: {gpu_info.stdout.strip()}\")\n",
        "\n",
        "# Install Google Drive API packages (for VSCode Colab integration)\n",
        "!pip install -q google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
        "\n",
        "# Uninstall existing torch packages to avoid version conflicts with Colab's pre-installed versions\n",
        "!pip uninstall -y torch torchvision torchaudio 2>/dev/null || true\n",
        "\n",
        "# Install pinned versions - these are tested compatible with xformers 0.0.28.post1\n",
        "!pip install torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install xformers==0.0.28.post1 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Clone kohya_ss sd-scripts\n",
        "KOHYA_DIR = \"/content/sd-scripts\"\n",
        "if not os.path.exists(KOHYA_DIR):\n",
        "    !git clone https://github.com/kohya-ss/sd-scripts.git {KOHYA_DIR}\n",
        "    %cd {KOHYA_DIR}\n",
        "    !git checkout sd3  # Use latest stable branch with SDXL support\n",
        "else:\n",
        "    %cd {KOHYA_DIR}\n",
        "    !git pull\n",
        "\n",
        "# Install kohya requirements\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "# Install compatible versions of key dependencies\n",
        "# Note: Using newer transformers/diffusers for compatibility with current xformers\n",
        "!pip install -q accelerate>=0.25.0 transformers>=4.36.2 diffusers>=0.25.1\n",
        "!pip install -q safetensors bitsandbytes>=0.41.3 prodigyopt lion-pytorch\n",
        "!pip install -q lycoris-lora\n",
        "!pip install -q \"rich<14\"  # Fix bigframes compatibility\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Installation complete!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JODJAtE0_D90",
        "outputId": "0d9f173d-d834-4505-e2bd-f860214e3ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticating with Google Drive API...\n",
            "credentials.json not found.\n",
            "\n",
            "Please upload your credentials.json file...\n",
            "(Download it from Google Cloud Console > APIs & Services > Credentials)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-260b3091-55ff-4bb5-b4dc-e3da229018b6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-260b3091-55ff-4bb5-b4dc-e3da229018b6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: No credentials file uploaded.\n"
          ]
        }
      ],
      "source": [
        "#@title 1.2 Connect to Google Drive (API Method)\n",
        "#@markdown Connect to Google Drive using the Drive API (works with VSCode Colab integration).\n",
        "#@markdown\n",
        "#@markdown **First-time setup:**\n",
        "#@markdown 1. Go to [Google Cloud Console](https://console.cloud.google.com/)\n",
        "#@markdown 2. Create a project and enable the Google Drive API\n",
        "#@markdown 3. Create OAuth 2.0 credentials (Desktop app type)\n",
        "#@markdown 4. Download `credentials.json` - you'll be prompted to upload it below\n",
        "\n",
        "import os\n",
        "import io\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# Full access scope for downloading and uploading files\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
        "\n",
        "def upload_credentials():\n",
        "    \"\"\"Prompt user to upload credentials.json file.\"\"\"\n",
        "    print(\"Please upload your credentials.json file...\")\n",
        "    print(\"(Download it from Google Cloud Console > APIs & Services > Credentials)\\n\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        # Get the uploaded filename (might not be exactly 'credentials.json')\n",
        "        uploaded_filename = list(uploaded.keys())[0]\n",
        "\n",
        "        # Move/rename to credentials.json in /content/\n",
        "        if uploaded_filename != 'credentials.json':\n",
        "            shutil.move(uploaded_filename, '/content/credentials.json')\n",
        "        else:\n",
        "            shutil.move('credentials.json', '/content/credentials.json')\n",
        "\n",
        "        print(f\"\\nCredentials file saved to /content/credentials.json\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def authenticate_drive():\n",
        "    \"\"\"Authenticate with Google Drive API and return service object.\"\"\"\n",
        "    creds = None\n",
        "\n",
        "    # Check for existing token\n",
        "    if os.path.exists('/content/token.json'):\n",
        "        creds = Credentials.from_authorized_user_file('/content/token.json', SCOPES)\n",
        "\n",
        "    # If no valid credentials, authenticate\n",
        "    if not creds or not creds.valid:\n",
        "        if creds and creds.expired and creds.refresh_token:\n",
        "            creds.refresh(Request())\n",
        "        else:\n",
        "            # Check if credentials.json exists, if not prompt for upload\n",
        "            if not os.path.exists('/content/credentials.json'):\n",
        "                print(\"credentials.json not found.\\n\")\n",
        "                if not upload_credentials():\n",
        "                    print(\"ERROR: No credentials file uploaded.\")\n",
        "                    return None\n",
        "\n",
        "            flow = InstalledAppFlow.from_client_secrets_file(\n",
        "                '/content/credentials.json', SCOPES)\n",
        "            creds = flow.run_local_server(port=0)\n",
        "\n",
        "        # Save credentials for future runs\n",
        "        with open('/content/token.json', 'w') as token:\n",
        "            token.write(creds.to_json())\n",
        "\n",
        "    return build('drive', 'v3', credentials=creds)\n",
        "\n",
        "def get_file_id_from_path(service, path):\n",
        "    \"\"\"Convert a Drive path like 'Loras/stoo_tee/dataset' to a file ID.\"\"\"\n",
        "    # Remove leading 'My Drive/' or '/content/drive/MyDrive/' if present\n",
        "    path = path.replace('/content/drive/MyDrive/', '')\n",
        "    path = path.replace('My Drive/', '')\n",
        "    path = path.lstrip('/')\n",
        "\n",
        "    parts = path.split('/')\n",
        "    parent_id = 'root'\n",
        "\n",
        "    for part in parts:\n",
        "        query = f\"name='{part}' and '{parent_id}' in parents and trashed=false\"\n",
        "        results = service.files().list(q=query, fields=\"files(id, name, mimeType)\").execute()\n",
        "        files_list = results.get('files', [])\n",
        "\n",
        "        if not files_list:\n",
        "            return None\n",
        "        parent_id = files_list[0]['id']\n",
        "\n",
        "    return parent_id\n",
        "\n",
        "def download_folder(service, drive_path, local_path):\n",
        "    \"\"\"Download a folder from Google Drive to local filesystem.\"\"\"\n",
        "    folder_id = get_file_id_from_path(service, drive_path)\n",
        "    if not folder_id:\n",
        "        print(f\"ERROR: Folder not found in Drive: {drive_path}\")\n",
        "        return False\n",
        "\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "\n",
        "    # List all files in folder\n",
        "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
        "    results = service.files().list(q=query, fields=\"files(id, name, mimeType)\").execute()\n",
        "    files_list = results.get('files', [])\n",
        "\n",
        "    print(f\"Downloading {len(files_list)} files from Drive...\")\n",
        "\n",
        "    for file in files_list:\n",
        "        file_path = os.path.join(local_path, file['name'])\n",
        "\n",
        "        if file['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "            # Recursively download subfolders\n",
        "            download_folder(service, f\"{drive_path}/{file['name']}\", file_path)\n",
        "        else:\n",
        "            # Download file\n",
        "            request = service.files().get_media(fileId=file['id'])\n",
        "            with open(file_path, 'wb') as f:\n",
        "                downloader = MediaIoBaseDownload(f, request)\n",
        "                done = False\n",
        "                while not done:\n",
        "                    status, done = downloader.next_chunk()\n",
        "            print(f\"  Downloaded: {file['name']}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def upload_file(service, local_path, drive_folder_path):\n",
        "    \"\"\"Upload a file to Google Drive folder.\"\"\"\n",
        "    folder_id = get_file_id_from_path(service, drive_folder_path)\n",
        "    if not folder_id:\n",
        "        # Create the folder if it doesn't exist\n",
        "        folder_id = create_drive_folder(service, drive_folder_path)\n",
        "\n",
        "    filename = os.path.basename(local_path)\n",
        "\n",
        "    # Check if file already exists\n",
        "    query = f\"name='{filename}' and '{folder_id}' in parents and trashed=false\"\n",
        "    results = service.files().list(q=query, fields=\"files(id)\").execute()\n",
        "    existing = results.get('files', [])\n",
        "\n",
        "    media = MediaFileUpload(local_path, resumable=True)\n",
        "\n",
        "    if existing:\n",
        "        # Update existing file\n",
        "        file = service.files().update(\n",
        "            fileId=existing[0]['id'],\n",
        "            media_body=media\n",
        "        ).execute()\n",
        "    else:\n",
        "        # Create new file\n",
        "        file_metadata = {'name': filename, 'parents': [folder_id]}\n",
        "        file = service.files().create(\n",
        "            body=file_metadata,\n",
        "            media_body=media,\n",
        "            fields='id'\n",
        "        ).execute()\n",
        "\n",
        "    return file.get('id')\n",
        "\n",
        "def create_drive_folder(service, path):\n",
        "    \"\"\"Create a folder path in Google Drive, creating parent folders as needed.\"\"\"\n",
        "    path = path.replace('/content/drive/MyDrive/', '').lstrip('/')\n",
        "    parts = path.split('/')\n",
        "    parent_id = 'root'\n",
        "\n",
        "    for part in parts:\n",
        "        # Check if folder exists\n",
        "        query = f\"name='{part}' and '{parent_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
        "        results = service.files().list(q=query, fields=\"files(id)\").execute()\n",
        "        files_list = results.get('files', [])\n",
        "\n",
        "        if files_list:\n",
        "            parent_id = files_list[0]['id']\n",
        "        else:\n",
        "            # Create folder\n",
        "            file_metadata = {\n",
        "                'name': part,\n",
        "                'mimeType': 'application/vnd.google-apps.folder',\n",
        "                'parents': [parent_id]\n",
        "            }\n",
        "            folder = service.files().create(body=file_metadata, fields='id').execute()\n",
        "            parent_id = folder.get('id')\n",
        "\n",
        "    return parent_id\n",
        "\n",
        "def upload_folder(service, local_path, drive_folder_path):\n",
        "    \"\"\"Upload all files from a local folder to Google Drive.\"\"\"\n",
        "    if not os.path.exists(local_path):\n",
        "        print(f\"ERROR: Local folder not found: {local_path}\")\n",
        "        return False\n",
        "\n",
        "    files_list = [f for f in os.listdir(local_path) if os.path.isfile(os.path.join(local_path, f))]\n",
        "    print(f\"Uploading {len(files_list)} files to Drive...\")\n",
        "\n",
        "    for filename in files_list:\n",
        "        filepath = os.path.join(local_path, filename)\n",
        "        upload_file(service, filepath, drive_folder_path)\n",
        "        print(f\"  Uploaded: {filename}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# Authenticate\n",
        "print(\"Authenticating with Google Drive API...\")\n",
        "drive_service = authenticate_drive()\n",
        "\n",
        "if drive_service:\n",
        "    # Test connection by listing root folder\n",
        "    results = drive_service.files().list(pageSize=5, fields=\"files(name)\").execute()\n",
        "    files_list = results.get('files', [])\n",
        "    print(\"\\nGoogle Drive connected successfully!\")\n",
        "    print(f\"Sample files in your Drive: {[f['name'] for f in files_list]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEz78Izj_D91"
      },
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Configure your training parameters. The defaults are optimized for T4 GPU (15GB VRAM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pUGc7y5W_D92",
        "outputId": "622927c1-1807-4a2f-e587-2cb0787ee9bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading training images from Google Drive...\n",
            "ERROR: Drive not authenticated. Run cell 1.2 first.\n",
            "\n",
            "Training Configuration:\n",
            "  - Drive source: Loras/stoo_tee/dataset\n",
            "  - Drive output: Loras/stoo_tee/output\n",
            "  - Local images: /content/training_images\n",
            "  - Local output: /content/output\n",
            "  - LoRA Name: stoo_tee\n",
            "  - Base Model: John6666/wai-ani-nsfw-ponyxl-v11-sdxl\n",
            "  - Network Dim: 32, Alpha: 16\n",
            "  - Trigger Word: stoo_tee\n",
            "  - Epochs: 10, Save every: 2\n"
          ]
        }
      ],
      "source": [
        "#@title 2.1 Training Configuration\n",
        "#@markdown ### Google Drive Paths (source/destination)\n",
        "DRIVE_TRAINING_FOLDER = \"Loras/stoo_tee/dataset\" #@param {type:\"string\"}\n",
        "DRIVE_OUTPUT_FOLDER = \"Loras/stoo_tee/output\" #@param {type:\"string\"}\n",
        "LORA_NAME = \"stoo_tee\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Base Model\n",
        "#@markdown Choose an SDXL base model for training\n",
        "BASE_MODEL = \"John6666/wai-ani-nsfw-ponyxl-v11-sdxl\" #@param [\"stabilityai/stable-diffusion-xl-base-1.0\", \"John6666/wai-ani-nsfw-ponyxl-v11-sdxl\", \"cagliostrolab/animagine-xl-3.1\"]\n",
        "\n",
        "#@markdown ### Training Parameters (T4 Optimized)\n",
        "NETWORK_DIM = 32 #@param {type:\"slider\", min:4, max:128, step:4}\n",
        "NETWORK_ALPHA = 16 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "LEARNING_RATE = 1e-4 #@param {type:\"number\"}\n",
        "UNET_LR = 1e-4 #@param {type:\"number\"}\n",
        "TEXT_ENCODER_LR = 5e-5 #@param {type:\"number\"}\n",
        "BATCH_SIZE = 1 #@param {type:\"slider\", min:1, max:4, step:1}\n",
        "MAX_TRAIN_EPOCHS = 10 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "SAVE_EVERY_N_EPOCHS = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "#@markdown ### Image Settings\n",
        "RESOLUTION = 512 #@param [512, 768, 1024] {type:\"raw\"}\n",
        "ENABLE_BUCKET = True #@param {type:\"boolean\"}\n",
        "MIN_BUCKET_RESO = 512 #@param {type:\"integer\"}\n",
        "MAX_BUCKET_RESO = 1536 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Trigger Word\n",
        "#@markdown A unique token to activate your likeness (e.g., \"ohwx\", \"sks\", your initials)\n",
        "TRIGGER_WORD = \"stoo_tee\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Optimizer (Prodigy recommended for automatic LR)\n",
        "OPTIMIZER = \"Prodigy\" #@param [\"AdamW8bit\", \"Prodigy\", \"Lion\", \"AdaFactor\"]\n",
        "\n",
        "#@markdown ### Advanced\n",
        "GRADIENT_CHECKPOINTING = True #@param {type:\"boolean\"}\n",
        "GRADIENT_ACCUMULATION = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "MIXED_PRECISION = \"fp16\" #@param [\"fp16\", \"bf16\", \"no\"]\n",
        "CACHE_LATENTS = True #@param {type:\"boolean\"}\n",
        "CACHE_TEXT_ENCODER = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Local paths (used during training)\n",
        "TRAINING_IMAGES_FOLDER = \"/content/training_images\"\n",
        "OUTPUT_FOLDER = \"/content/output\"\n",
        "\n",
        "# Create local directories\n",
        "import os\n",
        "os.makedirs(TRAINING_IMAGES_FOLDER, exist_ok=True)\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# Download training images from Google Drive\n",
        "print(\"Downloading training images from Google Drive...\")\n",
        "if drive_service:\n",
        "    success = download_folder(drive_service, DRIVE_TRAINING_FOLDER, TRAINING_IMAGES_FOLDER)\n",
        "    if success:\n",
        "        num_files = len([f for f in os.listdir(TRAINING_IMAGES_FOLDER) if not f.startswith('.')])\n",
        "        print(f\"\\nDownloaded {num_files} files to {TRAINING_IMAGES_FOLDER}\")\n",
        "    else:\n",
        "        print(\"Failed to download training images. Check the DRIVE_TRAINING_FOLDER path.\")\n",
        "else:\n",
        "    print(\"ERROR: Drive not authenticated. Run cell 1.2 first.\")\n",
        "\n",
        "print(f\"\\nTraining Configuration:\")\n",
        "print(f\"  - Drive source: {DRIVE_TRAINING_FOLDER}\")\n",
        "print(f\"  - Drive output: {DRIVE_OUTPUT_FOLDER}\")\n",
        "print(f\"  - Local images: {TRAINING_IMAGES_FOLDER}\")\n",
        "print(f\"  - Local output: {OUTPUT_FOLDER}\")\n",
        "print(f\"  - LoRA Name: {LORA_NAME}\")\n",
        "print(f\"  - Base Model: {BASE_MODEL}\")\n",
        "print(f\"  - Network Dim: {NETWORK_DIM}, Alpha: {NETWORK_ALPHA}\")\n",
        "print(f\"  - Trigger Word: {TRIGGER_WORD}\")\n",
        "print(f\"  - Epochs: {MAX_TRAIN_EPOCHS}, Save every: {SAVE_EVERY_N_EPOCHS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrnAYESE_D93"
      },
      "source": [
        "## 3. Data Preparation\n",
        "\n",
        "Validate and prepare your training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yvR5moFc_D93",
        "outputId": "f5c60b30-1860-4496-ec30-6beb686d375a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: No images found in /content/training_images\n",
            "\n",
            "Dataset validation FAILED. Please fix the issues above.\n"
          ]
        }
      ],
      "source": [
        "#@title 3.1 Validate Training Data\n",
        "#@markdown Check that your images and captions are properly formatted.\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "def validate_dataset(folder_path):\n",
        "    \"\"\"Validate training dataset structure and content.\"\"\"\n",
        "    folder = Path(folder_path)\n",
        "\n",
        "    if not folder.exists():\n",
        "        print(f\"ERROR: Folder not found: {folder_path}\")\n",
        "        print(\"Please check your TRAINING_IMAGES_FOLDER path.\")\n",
        "        return False\n",
        "\n",
        "    # Find all images\n",
        "    image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}\n",
        "    images = [f for f in folder.iterdir() if f.suffix.lower() in image_extensions]\n",
        "\n",
        "    if not images:\n",
        "        print(f\"ERROR: No images found in {folder_path}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Found {len(images)} images\\n\")\n",
        "\n",
        "    valid_pairs = []\n",
        "    missing_captions = []\n",
        "    invalid_images = []\n",
        "\n",
        "    for img_path in sorted(images):\n",
        "        # Check for matching caption file\n",
        "        caption_path = img_path.with_suffix('.txt')\n",
        "\n",
        "        # Validate image can be opened\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                width, height = img.size\n",
        "        except Exception as e:\n",
        "            invalid_images.append((img_path.name, str(e)))\n",
        "            continue\n",
        "\n",
        "        if caption_path.exists():\n",
        "            caption = caption_path.read_text().strip()\n",
        "            valid_pairs.append({\n",
        "                'image': img_path.name,\n",
        "                'caption': caption[:80] + '...' if len(caption) > 80 else caption,\n",
        "                'size': f\"{width}x{height}\"\n",
        "            })\n",
        "        else:\n",
        "            missing_captions.append(img_path.name)\n",
        "\n",
        "    # Report results\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"VALID IMAGE-CAPTION PAIRS: {len(valid_pairs)}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for pair in valid_pairs[:5]:  # Show first 5\n",
        "        print(f\"  {pair['image']} ({pair['size']})\")\n",
        "        print(f\"    Caption: {pair['caption']}\")\n",
        "\n",
        "    if len(valid_pairs) > 5:\n",
        "        print(f\"  ... and {len(valid_pairs) - 5} more\")\n",
        "\n",
        "    if missing_captions:\n",
        "        print(f\"\\nWARNING: {len(missing_captions)} images missing captions:\")\n",
        "        for name in missing_captions[:5]:\n",
        "            print(f\"  - {name}\")\n",
        "        if len(missing_captions) > 5:\n",
        "            print(f\"  ... and {len(missing_captions) - 5} more\")\n",
        "\n",
        "    if invalid_images:\n",
        "        print(f\"\\nERROR: {len(invalid_images)} invalid/corrupted images:\")\n",
        "        for name, err in invalid_images:\n",
        "            print(f\"  - {name}: {err}\")\n",
        "\n",
        "    # Recommendations\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"RECOMMENDATIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if len(valid_pairs) < 10:\n",
        "        print(\"  - Consider adding more images (15-30 recommended for likeness)\")\n",
        "    elif len(valid_pairs) > 50:\n",
        "        print(\"  - Large dataset detected. Consider reducing to 30-50 best images.\")\n",
        "    else:\n",
        "        print(f\"  - Dataset size ({len(valid_pairs)}) is good for likeness training\")\n",
        "\n",
        "    return len(valid_pairs) > 0\n",
        "\n",
        "# Run validation\n",
        "dataset_valid = validate_dataset(TRAINING_IMAGES_FOLDER)\n",
        "\n",
        "if dataset_valid:\n",
        "    print(\"\\nDataset validation PASSED. Ready to proceed.\")\n",
        "else:\n",
        "    print(\"\\nDataset validation FAILED. Please fix the issues above.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xb_pEbxx_D95",
        "outputId": "d2e9b0b7-1911-41af-b0cf-df9cc8371147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 0 image-caption pairs\n",
            "Dataset folder: /content/training_data/10_stoo_tee\n",
            "Repeats per image: 10\n",
            "\n",
            "Effective training steps per epoch: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3517472228.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Show sample caption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0msample_caption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_folder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nSample caption ({sample_caption.name}):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  {sample_caption.read_text()[:200]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "#@title 3.2 Prepare Dataset (Add Trigger Word to Captions)\n",
        "#@markdown This cell prepares the dataset by creating a proper folder structure\n",
        "#@markdown and optionally prepending the trigger word to all captions.\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "PREPEND_TRIGGER = True #@param {type:\"boolean\"}\n",
        "REPEATS = 10 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "\n",
        "# Create kohya-compatible folder structure\n",
        "# Format: <repeats>_<trigger_word>\n",
        "PREPARED_FOLDER = f\"/content/training_data/{REPEATS}_{TRIGGER_WORD}\"\n",
        "os.makedirs(PREPARED_FOLDER, exist_ok=True)\n",
        "\n",
        "source_folder = Path(TRAINING_IMAGES_FOLDER)\n",
        "dest_folder = Path(PREPARED_FOLDER)\n",
        "\n",
        "image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}\n",
        "images = [f for f in source_folder.iterdir() if f.suffix.lower() in image_extensions]\n",
        "\n",
        "processed = 0\n",
        "for img_path in images:\n",
        "    caption_path = img_path.with_suffix('.txt')\n",
        "    if not caption_path.exists():\n",
        "        continue\n",
        "\n",
        "    # Copy image\n",
        "    dest_img = dest_folder / img_path.name\n",
        "    shutil.copy2(img_path, dest_img)\n",
        "\n",
        "    # Process caption\n",
        "    caption = caption_path.read_text().strip()\n",
        "\n",
        "    if PREPEND_TRIGGER and not caption.lower().startswith(TRIGGER_WORD.lower()):\n",
        "        # Prepend trigger word\n",
        "        caption = f\"{TRIGGER_WORD} person, {caption}\"\n",
        "\n",
        "    # Write processed caption\n",
        "    dest_caption = dest_folder / img_path.with_suffix('.txt').name\n",
        "    dest_caption.write_text(caption)\n",
        "\n",
        "    processed += 1\n",
        "\n",
        "print(f\"Prepared {processed} image-caption pairs\")\n",
        "print(f\"Dataset folder: {PREPARED_FOLDER}\")\n",
        "print(f\"Repeats per image: {REPEATS}\")\n",
        "print(f\"\\nEffective training steps per epoch: {processed * REPEATS}\")\n",
        "\n",
        "# Show sample caption\n",
        "sample_caption = list(dest_folder.glob('*.txt'))[0]\n",
        "print(f\"\\nSample caption ({sample_caption.name}):\")\n",
        "print(f\"  {sample_caption.read_text()[:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4eoTpAV_D99"
      },
      "source": [
        "## 4. Training\n",
        "\n",
        "Run the LoRA training. This will take a while depending on your dataset size and epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hex3gyPz_D9-"
      },
      "outputs": [],
      "source": [
        "#@title 4.1 Generate Training Config\n",
        "#@markdown Creates the configuration file for kohya training.\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Build training arguments\n",
        "train_args = {\n",
        "    # Model\n",
        "    \"pretrained_model_name_or_path\": BASE_MODEL,\n",
        "    \"v2\": False,\n",
        "    \"v_parameterization\": False,\n",
        "\n",
        "    # Dataset\n",
        "    \"train_data_dir\": \"/content/training_data\",\n",
        "    \"resolution\": f\"{RESOLUTION},{RESOLUTION}\",\n",
        "    \"enable_bucket\": ENABLE_BUCKET,\n",
        "    \"min_bucket_reso\": MIN_BUCKET_RESO,\n",
        "    \"max_bucket_reso\": MAX_BUCKET_RESO,\n",
        "    \"bucket_reso_steps\": 64,\n",
        "\n",
        "    # Output\n",
        "    \"output_dir\": OUTPUT_FOLDER,\n",
        "    \"output_name\": LORA_NAME,\n",
        "    \"save_model_as\": \"safetensors\",\n",
        "    \"save_every_n_epochs\": SAVE_EVERY_N_EPOCHS,\n",
        "    \"save_precision\": \"fp16\",\n",
        "\n",
        "    # Network (LoRA)\n",
        "    \"network_module\": \"networks.lora\",\n",
        "    \"network_dim\": NETWORK_DIM,\n",
        "    \"network_alpha\": NETWORK_ALPHA,\n",
        "    \"network_train_unet_only\": False,\n",
        "    \"network_train_text_encoder_only\": False,\n",
        "\n",
        "    # Training\n",
        "    \"max_train_epochs\": MAX_TRAIN_EPOCHS,\n",
        "    \"train_batch_size\": BATCH_SIZE,\n",
        "    \"gradient_checkpointing\": GRADIENT_CHECKPOINTING,\n",
        "    \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION,\n",
        "    \"mixed_precision\": MIXED_PRECISION,\n",
        "\n",
        "    # Learning rates\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"unet_lr\": UNET_LR,\n",
        "    \"text_encoder_lr\": TEXT_ENCODER_LR,\n",
        "    \"lr_scheduler\": \"cosine_with_restarts\" if OPTIMIZER != \"Prodigy\" else \"constant\",\n",
        "    \"lr_warmup_steps\": 0 if OPTIMIZER == \"Prodigy\" else 100,\n",
        "    \"lr_scheduler_num_cycles\": 3,\n",
        "\n",
        "    # Optimizer\n",
        "    \"optimizer_type\": OPTIMIZER,\n",
        "\n",
        "    # Memory optimization\n",
        "    \"cache_latents\": CACHE_LATENTS,\n",
        "    \"cache_latents_to_disk\": False,\n",
        "    \"cache_text_encoder_outputs\": CACHE_TEXT_ENCODER,\n",
        "    \"cache_text_encoder_outputs_to_disk\": False,\n",
        "\n",
        "    # xformers for memory efficiency\n",
        "    \"xformers\": True,\n",
        "\n",
        "    # Shuffling and augmentation\n",
        "    \"shuffle_caption\": True,\n",
        "    \"keep_tokens\": 1,  # Keep trigger word at start\n",
        "    \"caption_extension\": \".txt\",\n",
        "\n",
        "    # SDXL specific\n",
        "    \"no_half_vae\": True,  # Prevent VAE issues with SDXL\n",
        "\n",
        "    # Logging\n",
        "    \"logging_dir\": \"/content/logs\",\n",
        "    \"log_with\": \"tensorboard\",\n",
        "\n",
        "    # Other\n",
        "    \"seed\": 42,\n",
        "    \"clip_skip\": 2,\n",
        "    \"max_token_length\": 225,\n",
        "}\n",
        "\n",
        "# Add Prodigy-specific settings\n",
        "if OPTIMIZER == \"Prodigy\":\n",
        "    train_args[\"optimizer_args\"] = [\n",
        "        \"decouple=True\",\n",
        "        \"weight_decay=0.01\",\n",
        "        \"d_coef=2\",\n",
        "        \"use_bias_correction=True\",\n",
        "        \"safeguard_warmup=True\",\n",
        "    ]\n",
        "    # Prodigy works best with LR=1\n",
        "    train_args[\"learning_rate\"] = 1.0\n",
        "    train_args[\"unet_lr\"] = 1.0\n",
        "    train_args[\"text_encoder_lr\"] = 1.0\n",
        "\n",
        "# Save config\n",
        "config_path = \"/content/training_config.json\"\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(train_args, f, indent=2)\n",
        "\n",
        "print(\"Training configuration saved!\")\n",
        "print(f\"\\nKey settings:\")\n",
        "print(f\"  - Base model: {BASE_MODEL}\")\n",
        "print(f\"  - Network dim: {NETWORK_DIM}, alpha: {NETWORK_ALPHA}\")\n",
        "print(f\"  - Optimizer: {OPTIMIZER}\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}, Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
        "print(f\"  - Effective batch: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
        "print(f\"  - Epochs: {MAX_TRAIN_EPOCHS}\")\n",
        "print(f\"  - Resolution: {RESOLUTION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJjLdEw0_D9_"
      },
      "outputs": [],
      "source": [
        "#@title 4.2 Start Training\n",
        "#@markdown This will train your LoRA. Monitor the loss values - they should decrease over time.\n",
        "#@markdown\n",
        "#@markdown **Expected training time on T4:**\n",
        "#@markdown - ~20 images, 10 epochs: 30-60 minutes\n",
        "#@markdown - ~30 images, 10 epochs: 45-90 minutes\n",
        "\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Load config\n",
        "with open('/content/training_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Build command line arguments\n",
        "cmd = [\"accelerate\", \"launch\", \"--num_cpu_threads_per_process=2\", \"sdxl_train_network.py\"]\n",
        "\n",
        "for key, value in config.items():\n",
        "    if isinstance(value, bool):\n",
        "        if value:\n",
        "            cmd.append(f\"--{key}\")\n",
        "    elif isinstance(value, list):\n",
        "        for item in value:\n",
        "            cmd.extend([f\"--{key}\", str(item)])\n",
        "    else:\n",
        "        cmd.extend([f\"--{key}\", str(value)])\n",
        "\n",
        "# Change to kohya directory\n",
        "%cd /content/sd-scripts\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Output will be saved to: {OUTPUT_FOLDER}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Run training\n",
        "!{' '.join(cmd)}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Upload trained LoRA files to Google Drive\n",
        "print(\"\\nUploading trained LoRA files to Google Drive...\")\n",
        "if drive_service:\n",
        "    lora_files = [f for f in os.listdir(OUTPUT_FOLDER) if f.endswith('.safetensors')]\n",
        "    if lora_files:\n",
        "        for lora_file in lora_files:\n",
        "            local_path = os.path.join(OUTPUT_FOLDER, lora_file)\n",
        "            upload_file(drive_service, local_path, DRIVE_OUTPUT_FOLDER)\n",
        "            print(f\"  Uploaded: {lora_file}\")\n",
        "        print(f\"\\nAll LoRA files uploaded to Drive: {DRIVE_OUTPUT_FOLDER}\")\n",
        "    else:\n",
        "        print(\"No LoRA files found to upload.\")\n",
        "else:\n",
        "    print(\"WARNING: Drive not connected. Files saved locally only.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAj0I9pv_D-A"
      },
      "outputs": [],
      "source": [
        "#@title 4.3 View Training Logs (TensorBoard)\n",
        "#@markdown Monitor training progress with TensorBoard.\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWv3jBiQ_D-C"
      },
      "source": [
        "## 5. Testing Your LoRA\n",
        "\n",
        "Generate test images using your newly trained LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG5PAC6s_D-C"
      },
      "outputs": [],
      "source": [
        "#@title 5.1 Load Pipeline with LoRA\n",
        "#@markdown Load the SDXL pipeline and apply your trained LoRA.\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler\n",
        "from safetensors.torch import load_file\n",
        "import os\n",
        "\n",
        "# Find the latest LoRA file\n",
        "lora_files = sorted(\n",
        "    [f for f in os.listdir(OUTPUT_FOLDER) if f.endswith('.safetensors')],\n",
        "    key=lambda x: os.path.getmtime(os.path.join(OUTPUT_FOLDER, x)),\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "if not lora_files:\n",
        "    print(\"ERROR: No LoRA files found in output folder!\")\n",
        "else:\n",
        "    LORA_PATH = os.path.join(OUTPUT_FOLDER, lora_files[0])\n",
        "    print(f\"Using LoRA: {lora_files[0]}\")\n",
        "\n",
        "    # Load base pipeline\n",
        "    print(\"\\nLoading SDXL pipeline...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        torch_dtype=torch.float16,\n",
        "        use_safetensors=True,\n",
        "        variant=\"fp16\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Set scheduler\n",
        "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
        "        pipe.scheduler.config,\n",
        "        use_karras_sigmas=True\n",
        "    )\n",
        "\n",
        "    # Load LoRA\n",
        "    print(f\"Loading LoRA from {LORA_PATH}...\")\n",
        "    pipe.load_lora_weights(LORA_PATH)\n",
        "\n",
        "    # Memory optimization\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "    print(\"\\nPipeline ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61PAKxne_D-E"
      },
      "outputs": [],
      "source": [
        "#@title 5.2 Generate Test Images\n",
        "#@markdown Generate images using your trained LoRA. Remember to include your trigger word!\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "#@markdown ### Prompt\n",
        "PROMPT = \"ohwx person, professional portrait photo, studio lighting, neutral background\" #@param {type:\"string\"}\n",
        "NEGATIVE_PROMPT = \"deformed, ugly, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, bad hands, poorly drawn hands, fused fingers, too many fingers\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Generation Settings\n",
        "NUM_IMAGES = 4 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "WIDTH = 1024 #@param {type:\"slider\", min:512, max:1536, step:64}\n",
        "HEIGHT = 1024 #@param {type:\"slider\", min:512, max:1536, step:64}\n",
        "GUIDANCE_SCALE = 7.5 #@param {type:\"slider\", min:1, max:20, step:0.5}\n",
        "NUM_STEPS = 30 #@param {type:\"slider\", min:10, max:50, step:5}\n",
        "LORA_SCALE = 0.8 #@param {type:\"slider\", min:0.1, max:1.5, step:0.1}\n",
        "\n",
        "# Set LoRA scale\n",
        "pipe.fuse_lora(lora_scale=LORA_SCALE)\n",
        "\n",
        "print(f\"Generating {NUM_IMAGES} images...\")\n",
        "print(f\"Prompt: {PROMPT}\")\n",
        "print(f\"LoRA Scale: {LORA_SCALE}\\n\")\n",
        "\n",
        "images = []\n",
        "for i in range(NUM_IMAGES):\n",
        "    seed = torch.randint(0, 2**32, (1,)).item()\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "    image = pipe(\n",
        "        prompt=PROMPT,\n",
        "        negative_prompt=NEGATIVE_PROMPT,\n",
        "        width=WIDTH,\n",
        "        height=HEIGHT,\n",
        "        guidance_scale=GUIDANCE_SCALE,\n",
        "        num_inference_steps=NUM_STEPS,\n",
        "        generator=generator,\n",
        "    ).images[0]\n",
        "\n",
        "    images.append((image, seed))\n",
        "    print(f\"  Generated image {i+1}/{NUM_IMAGES} (seed: {seed})\")\n",
        "\n",
        "# Unfuse for next generation with different scale\n",
        "pipe.unfuse_lora()\n",
        "\n",
        "# Display results\n",
        "cols = min(NUM_IMAGES, 4)\n",
        "rows = (NUM_IMAGES + cols - 1) // cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
        "\n",
        "if NUM_IMAGES == 1:\n",
        "    axes = [[axes]]\n",
        "elif rows == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, (image, seed) in enumerate(images):\n",
        "    row, col = idx // cols, idx % cols\n",
        "    axes[row][col].imshow(image)\n",
        "    axes[row][col].set_title(f\"Seed: {seed}\", fontsize=10)\n",
        "    axes[row][col].axis('off')\n",
        "\n",
        "# Hide empty subplots\n",
        "for idx in range(NUM_IMAGES, rows * cols):\n",
        "    row, col = idx // cols, idx % cols\n",
        "    axes[row][col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save test images locally and upload to Drive\n",
        "SAVE_TEST_IMAGES = True #@param {type:\"boolean\"}\n",
        "if SAVE_TEST_IMAGES:\n",
        "    test_output_dir = os.path.join(OUTPUT_FOLDER, \"test_images\")\n",
        "    os.makedirs(test_output_dir, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    saved_files = []\n",
        "    for idx, (image, seed) in enumerate(images):\n",
        "        filename = f\"{timestamp}_test_{idx:02d}_seed{seed}.png\"\n",
        "        filepath = os.path.join(test_output_dir, filename)\n",
        "        image.save(filepath)\n",
        "        saved_files.append(filepath)\n",
        "\n",
        "    print(f\"\\nTest images saved locally: {test_output_dir}\")\n",
        "\n",
        "    # Upload to Google Drive\n",
        "    if drive_service:\n",
        "        drive_test_folder = f\"{DRIVE_OUTPUT_FOLDER}/test_images\"\n",
        "        print(f\"Uploading test images to Drive: {drive_test_folder}\")\n",
        "        for filepath in saved_files:\n",
        "            upload_file(drive_service, filepath, drive_test_folder)\n",
        "            print(f\"  Uploaded: {os.path.basename(filepath)}\")\n",
        "        print(\"Upload complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gozPtrNp_D-F"
      },
      "outputs": [],
      "source": [
        "#@title 5.3 Copy LoRA to Final Location\n",
        "#@markdown Copy your best LoRA to a convenient location in Google Drive.\n",
        "\n",
        "import shutil\n",
        "\n",
        "FINAL_LORA_FOLDER = \"sdxl_loras\" #@param {type:\"string\"}\n",
        "\n",
        "# List available LoRA checkpoints (from local output folder)\n",
        "print(\"Available LoRA checkpoints:\")\n",
        "lora_files = sorted(\n",
        "    [f for f in os.listdir(OUTPUT_FOLDER) if f.endswith('.safetensors')]\n",
        ")\n",
        "for i, f in enumerate(lora_files):\n",
        "    size_mb = os.path.getsize(os.path.join(OUTPUT_FOLDER, f)) / (1024*1024)\n",
        "    print(f\"  [{i}] {f} ({size_mb:.1f} MB)\")\n",
        "\n",
        "#@markdown Select which checkpoint to copy (index number from list above)\n",
        "CHECKPOINT_INDEX = 0 #@param {type:\"integer\"}\n",
        "\n",
        "if lora_files and CHECKPOINT_INDEX < len(lora_files):\n",
        "    src = os.path.join(OUTPUT_FOLDER, lora_files[CHECKPOINT_INDEX])\n",
        "\n",
        "    if drive_service:\n",
        "        # Upload to final Drive location\n",
        "        print(f\"\\nUploading to Google Drive: {FINAL_LORA_FOLDER}/{lora_files[CHECKPOINT_INDEX]}\")\n",
        "        upload_file(drive_service, src, FINAL_LORA_FOLDER)\n",
        "        print(f\"Done! LoRA uploaded to Drive: {FINAL_LORA_FOLDER}\")\n",
        "    else:\n",
        "        print(\"ERROR: Drive not connected. Cannot upload to final location.\")\n",
        "elif not lora_files:\n",
        "    print(\"No LoRA files found. Run training first.\")\n",
        "else:\n",
        "    print(f\"Invalid index. Please choose 0-{len(lora_files)-1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n3RlIKI_D-H"
      },
      "source": [
        "## 6. Cleanup\n",
        "\n",
        "Free up resources when done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrRNqVuf_D-I"
      },
      "outputs": [],
      "source": [
        "#@title 6.1 Cleanup GPU Memory\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Delete pipeline\n",
        "try:\n",
        "    del pipe\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Show memory status\n",
        "!nvidia-smi\n",
        "\n",
        "print(\"\\nGPU memory cleared!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjVusIki_D-J"
      },
      "source": [
        "---\n",
        "\n",
        "## Tips for Better Results\n",
        "\n",
        "### Training Data Quality\n",
        "- Use **15-30 high-quality images** of the subject\n",
        "- Include variety: different angles, lighting, expressions, backgrounds\n",
        "- Avoid low-resolution, blurry, or heavily filtered images\n",
        "- Crop faces consistently if doing face-focused training\n",
        "\n",
        "### Captioning Tips\n",
        "- Start each caption with the trigger word (e.g., \"ohwx person\")\n",
        "- Be descriptive: \"ohwx person, professional headshot, studio lighting, wearing blue suit\"\n",
        "- Include relevant details: clothing, background, lighting, expression\n",
        "- Be consistent with terminology across captions\n",
        "\n",
        "### Training Parameters\n",
        "- **Network dim 32** is good for likeness, increase to 64 for more detail\n",
        "- **10-15 epochs** is usually sufficient for likeness\n",
        "- Lower **learning rate** (1e-5) if you see artifacts\n",
        "- Increase **repeats** if you have few images (<15)\n",
        "\n",
        "### Using the LoRA\n",
        "- Always include the trigger word in prompts\n",
        "- Start with **LoRA scale 0.7-0.8**, adjust as needed\n",
        "- Higher scale = stronger likeness but may reduce flexibility\n",
        "- Lower scale = more stylistic freedom but weaker likeness"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}