{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDXL LoRA Trainer for Likeness\n",
    "\n",
    "This notebook trains a LoRA (Low-Rank Adaptation) on SDXL to capture your likeness for use in image generation.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 compatible)\n",
    "- Training images stored in Google Drive with matching .txt caption files\n",
    "- ~15-30 high-quality photos of the subject\n",
    "\n",
    "**Training Data Format:**\n",
    "```\n",
    "your_folder/\n",
    "  image001.jpg\n",
    "  image001.txt  (caption describing the image)\n",
    "  image002.png\n",
    "  image002.txt\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "Install kohya_ss sd-scripts and dependencies. This takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.1 Install Dependencies\n",
    "#@markdown Run this cell first. Takes ~3-5 minutes.\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "gpu_info = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], \n",
    "                          capture_output=True, text=True)\n",
    "print(f\"GPU: {gpu_info.stdout.strip()}\")\n",
    "\n",
    "# Install base requirements\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q xformers==0.0.23.post1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Clone kohya_ss sd-scripts\n",
    "KOHYA_DIR = \"/content/sd-scripts\"\n",
    "if not os.path.exists(KOHYA_DIR):\n",
    "    !git clone https://github.com/kohya-ss/sd-scripts.git {KOHYA_DIR}\n",
    "    %cd {KOHYA_DIR}\n",
    "    !git checkout sd3  # Use latest stable branch with SDXL support\n",
    "else:\n",
    "    %cd {KOHYA_DIR}\n",
    "    !git pull\n",
    "\n",
    "# Install kohya requirements\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q accelerate==0.25.0 transformers==4.36.2 diffusers==0.25.1\n",
    "!pip install -q safetensors bitsandbytes==0.41.3 prodigyopt lion-pytorch\n",
    "!pip install -q lycoris-lora\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Installation complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.2 Mount Google Drive\n",
    "#@markdown Connect to your Google Drive to access training images.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify mount\n",
    "!ls /content/drive/MyDrive/ | head -10\n",
    "print(\"\\nGoogle Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configure your training parameters. The defaults are optimized for T4 GPU (15GB VRAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.1 Training Configuration\n",
    "#@markdown ### Paths\n",
    "TRAINING_IMAGES_FOLDER = \"/content/drive/MyDrive/lora_training/images\" #@param {type:\"string\"}\n",
    "OUTPUT_FOLDER = \"/content/drive/MyDrive/lora_training/output\" #@param {type:\"string\"}\n",
    "LORA_NAME = \"my_likeness\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Base Model\n",
    "#@markdown Choose an SDXL base model for training\n",
    "BASE_MODEL = \"stabilityai/stable-diffusion-xl-base-1.0\" #@param [\"stabilityai/stable-diffusion-xl-base-1.0\", \"John6666/wai-ani-nsfw-ponyxl-v11-sdxl\", \"cagliostrolab/animagine-xl-3.1\"]\n",
    "\n",
    "#@markdown ### Training Parameters (T4 Optimized)\n",
    "NETWORK_DIM = 32 #@param {type:\"slider\", min:4, max:128, step:4}\n",
    "NETWORK_ALPHA = 16 #@param {type:\"slider\", min:1, max:128, step:1}\n",
    "LEARNING_RATE = 1e-4 #@param {type:\"number\"}\n",
    "UNET_LR = 1e-4 #@param {type:\"number\"}\n",
    "TEXT_ENCODER_LR = 5e-5 #@param {type:\"number\"}\n",
    "BATCH_SIZE = 1 #@param {type:\"slider\", min:1, max:4, step:1}\n",
    "MAX_TRAIN_EPOCHS = 10 #@param {type:\"slider\", min:1, max:50, step:1}\n",
    "SAVE_EVERY_N_EPOCHS = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "\n",
    "#@markdown ### Image Settings\n",
    "RESOLUTION = 1024 #@param [512, 768, 1024] {type:\"raw\"}\n",
    "ENABLE_BUCKET = True #@param {type:\"boolean\"}\n",
    "MIN_BUCKET_RESO = 512 #@param {type:\"integer\"}\n",
    "MAX_BUCKET_RESO = 1536 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Trigger Word\n",
    "#@markdown A unique token to activate your likeness (e.g., \"ohwx\", \"sks\", your initials)\n",
    "TRIGGER_WORD = \"ohwx\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Optimizer (Prodigy recommended for automatic LR)\n",
    "OPTIMIZER = \"Prodigy\" #@param [\"AdamW8bit\", \"Prodigy\", \"Lion\", \"AdaFactor\"]\n",
    "\n",
    "#@markdown ### Advanced\n",
    "GRADIENT_CHECKPOINTING = True #@param {type:\"boolean\"}\n",
    "GRADIENT_ACCUMULATION = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
    "MIXED_PRECISION = \"fp16\" #@param [\"fp16\", \"bf16\", \"no\"]\n",
    "CACHE_LATENTS = True #@param {type:\"boolean\"}\n",
    "CACHE_TEXT_ENCODER = True #@param {type:\"boolean\"}\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  - Images: {TRAINING_IMAGES_FOLDER}\")\n",
    "print(f\"  - Output: {OUTPUT_FOLDER}\")\n",
    "print(f\"  - LoRA Name: {LORA_NAME}\")\n",
    "print(f\"  - Base Model: {BASE_MODEL}\")\n",
    "print(f\"  - Network Dim: {NETWORK_DIM}, Alpha: {NETWORK_ALPHA}\")\n",
    "print(f\"  - Trigger Word: {TRIGGER_WORD}\")\n",
    "print(f\"  - Epochs: {MAX_TRAIN_EPOCHS}, Save every: {SAVE_EVERY_N_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Validate and prepare your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.1 Validate Training Data\n",
    "#@markdown Check that your images and captions are properly formatted.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "def validate_dataset(folder_path):\n",
    "    \"\"\"Validate training dataset structure and content.\"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\"ERROR: Folder not found: {folder_path}\")\n",
    "        print(\"Please check your TRAINING_IMAGES_FOLDER path.\")\n",
    "        return False\n",
    "    \n",
    "    # Find all images\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}\n",
    "    images = [f for f in folder.iterdir() if f.suffix.lower() in image_extensions]\n",
    "    \n",
    "    if not images:\n",
    "        print(f\"ERROR: No images found in {folder_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Found {len(images)} images\\n\")\n",
    "    \n",
    "    valid_pairs = []\n",
    "    missing_captions = []\n",
    "    invalid_images = []\n",
    "    \n",
    "    for img_path in sorted(images):\n",
    "        # Check for matching caption file\n",
    "        caption_path = img_path.with_suffix('.txt')\n",
    "        \n",
    "        # Validate image can be opened\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                width, height = img.size\n",
    "        except Exception as e:\n",
    "            invalid_images.append((img_path.name, str(e)))\n",
    "            continue\n",
    "        \n",
    "        if caption_path.exists():\n",
    "            caption = caption_path.read_text().strip()\n",
    "            valid_pairs.append({\n",
    "                'image': img_path.name,\n",
    "                'caption': caption[:80] + '...' if len(caption) > 80 else caption,\n",
    "                'size': f\"{width}x{height}\"\n",
    "            })\n",
    "        else:\n",
    "            missing_captions.append(img_path.name)\n",
    "    \n",
    "    # Report results\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"VALID IMAGE-CAPTION PAIRS: {len(valid_pairs)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for pair in valid_pairs[:5]:  # Show first 5\n",
    "        print(f\"  {pair['image']} ({pair['size']})\")\n",
    "        print(f\"    Caption: {pair['caption']}\")\n",
    "    \n",
    "    if len(valid_pairs) > 5:\n",
    "        print(f\"  ... and {len(valid_pairs) - 5} more\")\n",
    "    \n",
    "    if missing_captions:\n",
    "        print(f\"\\nWARNING: {len(missing_captions)} images missing captions:\")\n",
    "        for name in missing_captions[:5]:\n",
    "            print(f\"  - {name}\")\n",
    "        if len(missing_captions) > 5:\n",
    "            print(f\"  ... and {len(missing_captions) - 5} more\")\n",
    "    \n",
    "    if invalid_images:\n",
    "        print(f\"\\nERROR: {len(invalid_images)} invalid/corrupted images:\")\n",
    "        for name, err in invalid_images:\n",
    "            print(f\"  - {name}: {err}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if len(valid_pairs) < 10:\n",
    "        print(\"  - Consider adding more images (15-30 recommended for likeness)\")\n",
    "    elif len(valid_pairs) > 50:\n",
    "        print(\"  - Large dataset detected. Consider reducing to 30-50 best images.\")\n",
    "    else:\n",
    "        print(f\"  - Dataset size ({len(valid_pairs)}) is good for likeness training\")\n",
    "    \n",
    "    return len(valid_pairs) > 0\n",
    "\n",
    "# Run validation\n",
    "dataset_valid = validate_dataset(TRAINING_IMAGES_FOLDER)\n",
    "\n",
    "if dataset_valid:\n",
    "    print(\"\\nDataset validation PASSED. Ready to proceed.\")\n",
    "else:\n",
    "    print(\"\\nDataset validation FAILED. Please fix the issues above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.2 Prepare Dataset (Add Trigger Word to Captions)\n",
    "#@markdown This cell prepares the dataset by creating a proper folder structure\n",
    "#@markdown and optionally prepending the trigger word to all captions.\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "PREPEND_TRIGGER = True #@param {type:\"boolean\"}\n",
    "REPEATS = 10 #@param {type:\"slider\", min:1, max:50, step:1}\n",
    "\n",
    "# Create kohya-compatible folder structure\n",
    "# Format: <repeats>_<trigger_word>\n",
    "PREPARED_FOLDER = f\"/content/training_data/{REPEATS}_{TRIGGER_WORD}\"\n",
    "os.makedirs(PREPARED_FOLDER, exist_ok=True)\n",
    "\n",
    "source_folder = Path(TRAINING_IMAGES_FOLDER)\n",
    "dest_folder = Path(PREPARED_FOLDER)\n",
    "\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}\n",
    "images = [f for f in source_folder.iterdir() if f.suffix.lower() in image_extensions]\n",
    "\n",
    "processed = 0\n",
    "for img_path in images:\n",
    "    caption_path = img_path.with_suffix('.txt')\n",
    "    if not caption_path.exists():\n",
    "        continue\n",
    "    \n",
    "    # Copy image\n",
    "    dest_img = dest_folder / img_path.name\n",
    "    shutil.copy2(img_path, dest_img)\n",
    "    \n",
    "    # Process caption\n",
    "    caption = caption_path.read_text().strip()\n",
    "    \n",
    "    if PREPEND_TRIGGER and not caption.lower().startswith(TRIGGER_WORD.lower()):\n",
    "        # Prepend trigger word\n",
    "        caption = f\"{TRIGGER_WORD} person, {caption}\"\n",
    "    \n",
    "    # Write processed caption\n",
    "    dest_caption = dest_folder / img_path.with_suffix('.txt').name\n",
    "    dest_caption.write_text(caption)\n",
    "    \n",
    "    processed += 1\n",
    "\n",
    "print(f\"Prepared {processed} image-caption pairs\")\n",
    "print(f\"Dataset folder: {PREPARED_FOLDER}\")\n",
    "print(f\"Repeats per image: {REPEATS}\")\n",
    "print(f\"\\nEffective training steps per epoch: {processed * REPEATS}\")\n",
    "\n",
    "# Show sample caption\n",
    "sample_caption = list(dest_folder.glob('*.txt'))[0]\n",
    "print(f\"\\nSample caption ({sample_caption.name}):\")\n",
    "print(f\"  {sample_caption.read_text()[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "\n",
    "Run the LoRA training. This will take a while depending on your dataset size and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Generate Training Config\n",
    "#@markdown Creates the configuration file for kohya training.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Build training arguments\n",
    "train_args = {\n",
    "    # Model\n",
    "    \"pretrained_model_name_or_path\": BASE_MODEL,\n",
    "    \"v2\": False,\n",
    "    \"v_parameterization\": False,\n",
    "    \n",
    "    # Dataset\n",
    "    \"train_data_dir\": \"/content/training_data\",\n",
    "    \"resolution\": f\"{RESOLUTION},{RESOLUTION}\",\n",
    "    \"enable_bucket\": ENABLE_BUCKET,\n",
    "    \"min_bucket_reso\": MIN_BUCKET_RESO,\n",
    "    \"max_bucket_reso\": MAX_BUCKET_RESO,\n",
    "    \"bucket_reso_steps\": 64,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": OUTPUT_FOLDER,\n",
    "    \"output_name\": LORA_NAME,\n",
    "    \"save_model_as\": \"safetensors\",\n",
    "    \"save_every_n_epochs\": SAVE_EVERY_N_EPOCHS,\n",
    "    \"save_precision\": \"fp16\",\n",
    "    \n",
    "    # Network (LoRA)\n",
    "    \"network_module\": \"networks.lora\",\n",
    "    \"network_dim\": NETWORK_DIM,\n",
    "    \"network_alpha\": NETWORK_ALPHA,\n",
    "    \"network_train_unet_only\": False,\n",
    "    \"network_train_text_encoder_only\": False,\n",
    "    \n",
    "    # Training\n",
    "    \"max_train_epochs\": MAX_TRAIN_EPOCHS,\n",
    "    \"train_batch_size\": BATCH_SIZE,\n",
    "    \"gradient_checkpointing\": GRADIENT_CHECKPOINTING,\n",
    "    \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION,\n",
    "    \"mixed_precision\": MIXED_PRECISION,\n",
    "    \n",
    "    # Learning rates\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"unet_lr\": UNET_LR,\n",
    "    \"text_encoder_lr\": TEXT_ENCODER_LR,\n",
    "    \"lr_scheduler\": \"cosine_with_restarts\" if OPTIMIZER != \"Prodigy\" else \"constant\",\n",
    "    \"lr_warmup_steps\": 0 if OPTIMIZER == \"Prodigy\" else 100,\n",
    "    \"lr_scheduler_num_cycles\": 3,\n",
    "    \n",
    "    # Optimizer\n",
    "    \"optimizer_type\": OPTIMIZER,\n",
    "    \n",
    "    # Memory optimization\n",
    "    \"cache_latents\": CACHE_LATENTS,\n",
    "    \"cache_latents_to_disk\": False,\n",
    "    \"cache_text_encoder_outputs\": CACHE_TEXT_ENCODER,\n",
    "    \"cache_text_encoder_outputs_to_disk\": False,\n",
    "    \n",
    "    # xformers for memory efficiency\n",
    "    \"xformers\": True,\n",
    "    \n",
    "    # Shuffling and augmentation\n",
    "    \"shuffle_caption\": True,\n",
    "    \"keep_tokens\": 1,  # Keep trigger word at start\n",
    "    \"caption_extension\": \".txt\",\n",
    "    \n",
    "    # SDXL specific\n",
    "    \"no_half_vae\": True,  # Prevent VAE issues with SDXL\n",
    "    \n",
    "    # Logging\n",
    "    \"logging_dir\": \"/content/logs\",\n",
    "    \"log_with\": \"tensorboard\",\n",
    "    \n",
    "    # Other\n",
    "    \"seed\": 42,\n",
    "    \"clip_skip\": 2,\n",
    "    \"max_token_length\": 225,\n",
    "}\n",
    "\n",
    "# Add Prodigy-specific settings\n",
    "if OPTIMIZER == \"Prodigy\":\n",
    "    train_args[\"optimizer_args\"] = [\n",
    "        \"decouple=True\",\n",
    "        \"weight_decay=0.01\",\n",
    "        \"d_coef=2\",\n",
    "        \"use_bias_correction=True\",\n",
    "        \"safeguard_warmup=True\",\n",
    "    ]\n",
    "    # Prodigy works best with LR=1\n",
    "    train_args[\"learning_rate\"] = 1.0\n",
    "    train_args[\"unet_lr\"] = 1.0\n",
    "    train_args[\"text_encoder_lr\"] = 1.0\n",
    "\n",
    "# Save config\n",
    "config_path = \"/content/training_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(train_args, f, indent=2)\n",
    "\n",
    "print(\"Training configuration saved!\")\n",
    "print(f\"\\nKey settings:\")\n",
    "print(f\"  - Base model: {BASE_MODEL}\")\n",
    "print(f\"  - Network dim: {NETWORK_DIM}, alpha: {NETWORK_ALPHA}\")\n",
    "print(f\"  - Optimizer: {OPTIMIZER}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}, Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  - Effective batch: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  - Epochs: {MAX_TRAIN_EPOCHS}\")\n",
    "print(f\"  - Resolution: {RESOLUTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Start Training\n",
    "#@markdown This will train your LoRA. Monitor the loss values - they should decrease over time.\n",
    "#@markdown \n",
    "#@markdown **Expected training time on T4:**\n",
    "#@markdown - ~20 images, 10 epochs: 30-60 minutes\n",
    "#@markdown - ~30 images, 10 epochs: 45-90 minutes\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Load config\n",
    "with open('/content/training_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Build command line arguments\n",
    "cmd = [\"accelerate\", \"launch\", \"--num_cpu_threads_per_process=2\", \"sdxl_train_network.py\"]\n",
    "\n",
    "for key, value in config.items():\n",
    "    if isinstance(value, bool):\n",
    "        if value:\n",
    "            cmd.append(f\"--{key}\")\n",
    "    elif isinstance(value, list):\n",
    "        for item in value:\n",
    "            cmd.extend([f\"--{key}\", str(item)])\n",
    "    else:\n",
    "        cmd.extend([f\"--{key}\", str(value)])\n",
    "\n",
    "# Change to kohya directory\n",
    "%cd /content/sd-scripts\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Output will be saved to: {OUTPUT_FOLDER}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run training\n",
    "!{' '.join(cmd)}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.3 View Training Logs (TensorBoard)\n",
    "#@markdown Monitor training progress with TensorBoard.\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing Your LoRA\n",
    "\n",
    "Generate test images using your newly trained LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.1 Load Pipeline with LoRA\n",
    "#@markdown Load the SDXL pipeline and apply your trained LoRA.\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "# Find the latest LoRA file\n",
    "lora_files = sorted(\n",
    "    [f for f in os.listdir(OUTPUT_FOLDER) if f.endswith('.safetensors')],\n",
    "    key=lambda x: os.path.getmtime(os.path.join(OUTPUT_FOLDER, x)),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "if not lora_files:\n",
    "    print(\"ERROR: No LoRA files found in output folder!\")\n",
    "else:\n",
    "    LORA_PATH = os.path.join(OUTPUT_FOLDER, lora_files[0])\n",
    "    print(f\"Using LoRA: {lora_files[0]}\")\n",
    "    \n",
    "    # Load base pipeline\n",
    "    print(\"\\nLoading SDXL pipeline...\")\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Set scheduler\n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "        pipe.scheduler.config,\n",
    "        use_karras_sigmas=True\n",
    "    )\n",
    "    \n",
    "    # Load LoRA\n",
    "    print(f\"Loading LoRA from {LORA_PATH}...\")\n",
    "    pipe.load_lora_weights(LORA_PATH)\n",
    "    \n",
    "    # Memory optimization\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    \n",
    "    print(\"\\nPipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.2 Generate Test Images\n",
    "#@markdown Generate images using your trained LoRA. Remember to include your trigger word!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "#@markdown ### Prompt\n",
    "PROMPT = \"ohwx person, professional portrait photo, studio lighting, neutral background\" #@param {type:\"string\"}\n",
    "NEGATIVE_PROMPT = \"deformed, ugly, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, bad hands, poorly drawn hands, fused fingers, too many fingers\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Generation Settings\n",
    "NUM_IMAGES = 4 #@param {type:\"slider\", min:1, max:8, step:1}\n",
    "WIDTH = 1024 #@param {type:\"slider\", min:512, max:1536, step:64}\n",
    "HEIGHT = 1024 #@param {type:\"slider\", min:512, max:1536, step:64}\n",
    "GUIDANCE_SCALE = 7.5 #@param {type:\"slider\", min:1, max:20, step:0.5}\n",
    "NUM_STEPS = 30 #@param {type:\"slider\", min:10, max:50, step:5}\n",
    "LORA_SCALE = 0.8 #@param {type:\"slider\", min:0.1, max:1.5, step:0.1}\n",
    "\n",
    "# Set LoRA scale\n",
    "pipe.fuse_lora(lora_scale=LORA_SCALE)\n",
    "\n",
    "print(f\"Generating {NUM_IMAGES} images...\")\n",
    "print(f\"Prompt: {PROMPT}\")\n",
    "print(f\"LoRA Scale: {LORA_SCALE}\\n\")\n",
    "\n",
    "images = []\n",
    "for i in range(NUM_IMAGES):\n",
    "    seed = torch.randint(0, 2**32, (1,)).item()\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt=PROMPT,\n",
    "        negative_prompt=NEGATIVE_PROMPT,\n",
    "        width=WIDTH,\n",
    "        height=HEIGHT,\n",
    "        guidance_scale=GUIDANCE_SCALE,\n",
    "        num_inference_steps=NUM_STEPS,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    images.append((image, seed))\n",
    "    print(f\"  Generated image {i+1}/{NUM_IMAGES} (seed: {seed})\")\n",
    "\n",
    "# Unfuse for next generation with different scale\n",
    "pipe.unfuse_lora()\n",
    "\n",
    "# Display results\n",
    "cols = min(NUM_IMAGES, 4)\n",
    "rows = (NUM_IMAGES + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
    "\n",
    "if NUM_IMAGES == 1:\n",
    "    axes = [[axes]]\n",
    "elif rows == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (image, seed) in enumerate(images):\n",
    "    row, col = idx // cols, idx % cols\n",
    "    axes[row][col].imshow(image)\n",
    "    axes[row][col].set_title(f\"Seed: {seed}\", fontsize=10)\n",
    "    axes[row][col].axis('off')\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(NUM_IMAGES, rows * cols):\n",
    "    row, col = idx // cols, idx % cols\n",
    "    axes[row][col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to Drive\n",
    "SAVE_TEST_IMAGES = True #@param {type:\"boolean\"}\n",
    "if SAVE_TEST_IMAGES:\n",
    "    test_output_dir = os.path.join(OUTPUT_FOLDER, \"test_images\")\n",
    "    os.makedirs(test_output_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    for idx, (image, seed) in enumerate(images):\n",
    "        filename = f\"{timestamp}_test_{idx:02d}_seed{seed}.png\"\n",
    "        image.save(os.path.join(test_output_dir, filename))\n",
    "    \n",
    "    print(f\"\\nTest images saved to: {test_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.3 Copy LoRA to Final Location\n",
    "#@markdown Copy your best LoRA to a convenient location in Google Drive.\n",
    "\n",
    "import shutil\n",
    "\n",
    "FINAL_LORA_FOLDER = \"/content/drive/MyDrive/sdxl_loras\" #@param {type:\"string\"}\n",
    "\n",
    "os.makedirs(FINAL_LORA_FOLDER, exist_ok=True)\n",
    "\n",
    "# List available LoRA checkpoints\n",
    "print(\"Available LoRA checkpoints:\")\n",
    "lora_files = sorted(\n",
    "    [f for f in os.listdir(OUTPUT_FOLDER) if f.endswith('.safetensors')]\n",
    ")\n",
    "for i, f in enumerate(lora_files):\n",
    "    size_mb = os.path.getsize(os.path.join(OUTPUT_FOLDER, f)) / (1024*1024)\n",
    "    print(f\"  [{i}] {f} ({size_mb:.1f} MB)\")\n",
    "\n",
    "#@markdown Select which checkpoint to copy (index number from list above)\n",
    "CHECKPOINT_INDEX = 0 #@param {type:\"integer\"}\n",
    "\n",
    "if CHECKPOINT_INDEX < len(lora_files):\n",
    "    src = os.path.join(OUTPUT_FOLDER, lora_files[CHECKPOINT_INDEX])\n",
    "    dst = os.path.join(FINAL_LORA_FOLDER, lora_files[CHECKPOINT_INDEX])\n",
    "    \n",
    "    shutil.copy2(src, dst)\n",
    "    print(f\"\\nCopied to: {dst}\")\n",
    "else:\n",
    "    print(f\"Invalid index. Please choose 0-{len(lora_files)-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "Free up resources when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.1 Cleanup GPU Memory\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Delete pipeline\n",
    "try:\n",
    "    del pipe\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Show memory status\n",
    "!nvidia-smi\n",
    "\n",
    "print(\"\\nGPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tips for Better Results\n",
    "\n",
    "### Training Data Quality\n",
    "- Use **15-30 high-quality images** of the subject\n",
    "- Include variety: different angles, lighting, expressions, backgrounds\n",
    "- Avoid low-resolution, blurry, or heavily filtered images\n",
    "- Crop faces consistently if doing face-focused training\n",
    "\n",
    "### Captioning Tips\n",
    "- Start each caption with the trigger word (e.g., \"ohwx person\")\n",
    "- Be descriptive: \"ohwx person, professional headshot, studio lighting, wearing blue suit\"\n",
    "- Include relevant details: clothing, background, lighting, expression\n",
    "- Be consistent with terminology across captions\n",
    "\n",
    "### Training Parameters\n",
    "- **Network dim 32** is good for likeness, increase to 64 for more detail\n",
    "- **10-15 epochs** is usually sufficient for likeness\n",
    "- Lower **learning rate** (1e-5) if you see artifacts\n",
    "- Increase **repeats** if you have few images (<15)\n",
    "\n",
    "### Using the LoRA\n",
    "- Always include the trigger word in prompts\n",
    "- Start with **LoRA scale 0.7-0.8**, adjust as needed\n",
    "- Higher scale = stronger likeness but may reduce flexibility\n",
    "- Lower scale = more stylistic freedom but weaker likeness"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
